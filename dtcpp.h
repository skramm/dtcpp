/**
\file
\brief Naive implementation attempt of a classifier using a Decision Tree
for continuous data values (aka real numbers).
See doc on https://github.com/skramm/dtcpp
\author S. Kramm - 2021
\copyright GPLv3

- home: https://github.com/skramm/dtcpp
- multiclass
- Limited to binary classification (a tree node has only two childs)
- input datasets:
	- csv style
	- field separator can be defined, see Fparams
	- class field MUST be the last one
	- number of attributes set automatically
	- classes may be integer values or string values, see Fparams
- Does not handle missing values
- using boost::graph to implement the tree
*/

#ifndef DTCPP_HG
#define DTCPP_HG

#include <iostream>
#include <fstream>
#include <vector>
#include <numeric>
#include <algorithm>
#include <random>
#include <iomanip>
#include <chrono>

#include <boost/graph/adjacency_list.hpp>
//#include <boost/graph/graph_utility.hpp> // needed only for print_graph();

#include <boost/histogram.hpp>

/// All the API and code lies here
namespace dtcpp {

#ifdef DEBUG
	#define COUT if(1) std::cout << __FUNCTION__ << "(), line " << __LINE__ << ": "
#else
	#define COUT if(0) std::cout
#endif // DEBUG

#ifdef DEBUG_START
	#define START if(1) std::cout << "* Start: " << __FUNCTION__ << "()\n"
#else
	#define START
#endif // DEBUG


#define LOG( level, msg ) \
	{ \
		if( g_params.verbose && level<=g_params.verboseLevel ) \
		{ \
			std::cout << std::setfill('0') << std::setw(4) << g_params.timer.getDuration(level); \
			priv::spaceLog( level ); \
			std::cout << " E" << std::setfill('0') << std::setw(4) << priv::logCount()++ << '-' << __FUNCTION__ << "(): " << msg << '\n'; \
		} \
	}

#define TIMER_START g_params.timer.start()


// % % % % % % % % % % % % % %
/// private namespace; not part of API
namespace priv {
// % % % % % % % % % % % % % %

uint& logCount()
{
	static uint s_logCount;
	return s_logCount;
}

//---------------------------------------------------------------------
/// Used in logging macro, see \ref LOG
void spaceLog( int n )
{
	std::cout << ':';
	for( int i=0; i<n; i++ )
		std::cout << "  ";
}

//---------------------------------------------------------------------
/// Holds timing
/// \todo add level to have a timing PER log level
struct Timer
{
//auto t1 = std::chrono::high_resolution_clock::now();
	std::string getDuration(int level)
	{
		auto t2 = std::chrono::high_resolution_clock::now();
		auto tdiff = std::chrono::duration_cast<std::chrono::milliseconds>( t2 - ck ).count();
		ck = t2;
		return std::to_string( tdiff );
	}
	void start()
	{
		ck = std::chrono::high_resolution_clock::now();
	}
	std::chrono::high_resolution_clock::time_point ck;
};

//---------------------------------------------------------------------
struct Gparams
{
	bool  verbose = false;
	int   verboseLevel = 1;
	Timer timer; ///< Used for logging, to measure duration.
};


/// Root string used to generated the script that will be used to generate plots of the attribute histograms,
/// see DataSet::computeStats()
std::string g_root_gnuplot_histo =
	"set style data histogram\n \
	set style histogram cluster gap 1\n \
	set style fill solid\n \
	set boxwidth 1\n";

//---------------------------------------------------------------------
enum EN_FileType
{
	FT_CSV,FT_DAT,FT_HTML,FT_PLT,FT_DOT
};
const char*
getString( EN_FileType ft )
{
	const char* s=0;
	switch( ft )
	{
		case FT_CSV:  s = "csv";  break;
		case FT_DAT:  s = "dat";  break;
		case FT_PLT:  s = "plt";  break;
		case FT_DOT:  s = "dot";  break;
		case FT_HTML: s = "html"; break;
		default: assert(0);
	}
	return s;
}
//---------------------------------------------------------------------
/// Generic function used to open output files
auto
openOutputFile( std::string fn, EN_FileType ft, std::string data_fn=std::string() )
{
	std::ostringstream oss;
	oss << "out/" << fn << '.' << getString( ft );
	auto fname = oss.str();
	std::ofstream f(fname);
	if( !f.is_open() )
		throw std::runtime_error( "unable to open file " + fname );
	if( ft == FT_PLT )
		f << "#!/usr/bin/env gnuplot\n\n";

	f << "# GENERATED FILE, DO NOT EDIT !\n"
		<< "# generated by dtcpp, see https://github.com/skramm/dtcpp\n";
	{
		auto now = std::chrono::system_clock::now();
		auto in_time_t = std::chrono::system_clock::to_time_t(now);

		std::ostringstream ss;
		ss << std::put_time(std::localtime(&in_time_t), "%Y-%m-%d %X");
		f << "# generated on " << ss.str() << "\n";
	}
	if( !data_fn.empty() )
		f << "# source data file: " << data_fn << '\n';
	f << '\n';
	if( ft == FT_PLT )
		f << "set terminal pngcairo\n";
	return f;
}

//---------------------------------------------------------------------
/// A template to have strong types, taken from J. Boccara
/**
 * https://www.fluentcpp.com/2016/12/08/strong-types-for-strong-interfaces/
*/
template <typename T, typename Parameter>
class NamedType
{
public:
	NamedType() {}
	explicit NamedType(T const& value) : value_(value) {}
	explicit NamedType(T&& value) : value_(std::move(value)) {}
	T&       get()       { return value_; }
	T const& get() const { return value_; }
	bool operator == ( const NamedType& nt2 ) const
	{
		return get() == nt2.get();
	}
	bool operator < ( const NamedType& nt2 ) const
	{
		return get() < nt2.get();
	}
	bool operator != ( const NamedType& nt2 ) const
	{
		return !( *this == nt2 );
	}

	friend std::ostream& operator << ( std::ostream& f, const NamedType& nt )
	{
		f << nt.value_;
		return f;
	}
private:
	T value_;
};


// forward declaration
//template<typename U>
//class DataSet;

//---------------------------------------------------------------------
/// General utility function
template<typename T>
void
printVector( std::ostream& f, const std::vector<T>& vec, const char* msg=0, bool lineBreak=false )
{
	f << "Vector: ";
	if( msg )
		f << msg;
	f << " #=" << vec.size() << ":\n";
	for( const auto& elem : vec )
	{
		f << elem;
		if( lineBreak )
			f << '\n';
		else
			f << "-";
	}
	f << '\n';
}
/// General utility function
template<typename K, typename V>
void
printMap( std::ostream& f, const std::map<K,V>& m, const char* msg=0 )
{
	f << "Map: ";
	if( msg )
		f << msg;
	f << " #=" << m.size() << ":\n";
	for( const auto& elem : m )
		f << " -" << elem.first << "-" << elem.second << '\n';
	f << "\n";
}

//-------------------------------------------------------------------
/// Remove multiple spaces AND TABS in string, allows only one, except if in first position
/**
Also replaces tabs with spaces
*/
std::string
trimSpaces( const std::string& input )
{
	assert( input.size() > 0 );
	bool HasOneAlready( false );
	bool FirstElem( true );
	std::string out;
	for( auto c: input )
	{
		if( c != ' ' && c != 9 )
		{
			out.push_back( c );
			HasOneAlready = false;
			FirstElem = false;
		}
		else {
			if( !HasOneAlready && !FirstElem )
			{
				out.push_back( ' ' ); // add a space character
				HasOneAlready = true;
			}
		}
	}
	if( out.back() == ' ' ) // if last element is a space, then remove it
		out.erase( out.end()-1 );

	return out;
}

//---------------------------------------------------------------------
/// General string tokenizer, taken from http://stackoverflow.com/a/236803/193789
/**
- see also this one: http://stackoverflow.com/a/53878/193789
*/
inline
std::vector<std::string>
splitString( const std::string &s, char delim )
{
	std::vector<std::string> velems;
	std::stringstream ss( trimSpaces(s) );
	std::string item;
	while( std::getline( ss, item, delim ) )
		velems.push_back(item);

	return velems;
}

//---------------------------------------------------------------------
/// Edge of the tree. Value is true/false of the above decision, depending on threshold
struct EdgeData
{
	bool edgeSide;
};

// % % % % % % % % % % % % % %
} // namespace priv
// % % % % % % % % % % % % % %

//---------------------------------------------------------------------
/// Global parameters
priv::Gparams g_params;

using ThresholdVal = priv::NamedType<float,struct ThresholdValTag>;
using ClassVal     = priv::NamedType<int,  struct ClassValTag>;

//---------------------------------------------------------------------
/// Run-time parameters for training
struct Params
{
	float minGiniCoeffForSplitting = 0.05f;
	uint  minNbPoints = 3;                   ///< minimum nb of points to create a node
	float removalCoeff = 0.05f;  ///< used to remove close attribute values when searching the best threshold. See removeDuplicates()
//	bool  verbose = true;        ///< to allow logging of some run-time details
//	int   verboseLevel = 0;      ///< verbose Level, related to \ref verbose
//	bool  doFolding = false;
//	int   nbFolds = 5;
	uint  maxTreeDepth = 12;
};

//---------------------------------------------------------------------
/// Preliminar stuff, WIP
namespace prelim {

enum DataType
{
	DT_real, DT_integer, DT_string, DT_bool
};

class DataSetDescription
{
public:
		DataSetDescription( uint nbAttribs )
			: _dataType(nbAttribs)
		{}
	private:
		std::vector<DataType> _dataType;
};

} // namespace prelim {

//---------------------------------------------------------------------
/// A datapoint, holds a set of attributes value and a corresponding (binary) class
//template<typename T>
class DataPoint
{
//	template<typename U>
	friend class DataSet;

	private:
		std::vector<float> _attrValue;   ///< attributes
		ClassVal _class = ClassVal(-1);  ///< Class of the datapoint, -1 for undefined

	public:
#ifdef TESTMODE
/// Constructor used in tests
		DataPoint( const std::vector<float>& vec, int c ) :
			_attrValue(vec), _class(ClassVal(c))
		{}
#endif
/// Constructor from a vector of strings (used by file reader)
		DataPoint( const std::vector<std::string>& v_string, ClassVal c ) : DataPoint( v_string )
		{
			_class = c;
		}
/// Constructor from a vector of strings (used by file reader when no class is given)
		DataPoint( const std::vector<std::string>& v_string )
		{
			assert( v_string.size() > 0 );              // at least one attribute
			try
			{
				for( size_t i=0; i<v_string.size(); i++ )
					_attrValue.push_back( std::stof( v_string[i] ) );
			}
			catch(...)
			{
				priv::printVector( std::cerr, v_string, "string conversion error", false );
				throw std::runtime_error( "unable to convert a string value to float" );
			}
		}

/// Constructor from a vector of floats and a class value
		DataPoint( const std::vector<float>& v_val, ClassVal c ) : DataPoint( v_val )
		{
			_class = c;
		}
/// Constructor from a vector of floats and no class value
		DataPoint( const std::vector<float>& v_val )
		{
			assert( v_val.size() > 0 );              // at least one attribute

			for( size_t i=0; i<v_val.size(); i++ )
				_attrValue.push_back( v_val[i] );
		}

		size_t nbAttribs() const
		{
			return _attrValue.size();
		}
		ClassVal classVal() const { return _class; }
		void setSize( size_t n ) { _attrValue.resize(n); }

		const float& attribVal( size_t idx ) const
		{
// TEMP
			if( idx>=_attrValue.size() )
				std::cerr << "idx=" << idx << " _attrValue.size()=" << _attrValue.size() << "\n";
			assert( idx<_attrValue.size() );
			return _attrValue[idx];
		}
		float& attribVal( size_t idx )
		{
			assert( idx<_attrValue.size() );
			return _attrValue[idx];
		}

//		template<typename U>
		void setAttribVector( const std::vector<float>& vec )
		{
			assert( vec.size() == nbAttribs() );
			_attrValue = vec;
		}
		void print( std::ostream& f ) const
		{
			for( const auto& v: _attrValue )
				f << v << ' ';
			f << classVal() << '\n';
		}
		friend std::ostream& operator << ( std::ostream& f, const DataPoint& pt )
		{
			f << "Datapoint: ";
			for( const auto& v: pt._attrValue )
				f << v << '-';
			f << "C=" << pt._class.get() << ' ';
			return f;
		}
};

//---------------------------------------------------------------------
/// Parameters for reading data files
struct Fparams
{
	char sep = ' ';                   ///< input field separator
	bool classAsString = false;       ///< class values are given as strings
	bool dataFilesHoldsClass = true;  ///< set to false to read files holding only attribute values (for classification task)
	bool classIsfirst = false;        ///< default: class is last element of line, if first, then set this to true
	bool firstLineHoldsName = false;  ///< unused \todo implement this
	uint nbBinHistograms = 15;        ///< Nb of bins for the data analysis histograms
};

//---------------------------------------------------------------------
/// Stats for a single attribute, see DataSet::computeStats()
template<typename T>
struct AttribStats
{
	T _minVal;
	T _maxVal;
	T _meanVal;
	T _stddevVal;
	T _medianVal;

	friend std::ostream& operator << ( std::ostream& f, const AttribStats& st )
	{
		f << "min="      << st._minVal
			<< " max="    << st._maxVal
			<< " range="  << st._maxVal - st._minVal
			<< " mean="   << st._meanVal
			<< " stddev=" << st._stddevVal
			<< " median=" << st._medianVal
			<< " ratio stddev/range=" << 100. * st._stddevVal / (st._maxVal - st._minVal)
			<< ' ';
		return f;
	}
};
//---------------------------------------------------------------------
/// Holds attribute stats, see DataSet::computeStats() (just a wrapper, actually...)
template<typename T>
struct DatasetStats
{
	private:
		std::vector<AttribStats<T>> v_stats;

	public:
/// Constructor. Argument is the number of attributes
		DatasetStats( size_t nbAttribs ) : v_stats(nbAttribs)
		{}
		void add( size_t idx, const AttribStats<T>& ats )
		{
			v_stats[idx] = ats;
		}
		AttribStats<T> get( size_t idx ) const
		{
			assert( idx < v_stats.size() );
			return v_stats[idx];
		}
		friend std::ostream& operator << ( std::ostream& f, const DatasetStats<T>& st )
		{
			f << "DatasetStats: " << st.v_stats.size() << " attributes:"; // << st.nbClasses() << '\n';
			for( uint i=0; i<st.v_stats.size(); i++ )
				f << "\n -attribute " << i << ": " << st.v_stats[i];
			f << '\n';
			return f;
		}
};
//---------------------------------------------------------------------
/// Used in TrainingTree to map a class value to an index in the \ref ConfusionMatrix
using ClassIndexMap = std::map<ClassVal,size_t>;

//---------------------------------------------------------------------
/// Outlier Detection Method. Related to Dataset::tagOutliers()
enum class En_OD_method
{
	fixedSigma
	,ChauvenetCrit ///< https://en.wikipedia.org/wiki/Chauvenet%27s_criterion
};
//---------------------------------------------------------------------
/// Outlier Removal Method. Related to Dataset::tagOutliers()
enum class En_OR_method
{
 	disablePoint       ///< tag the point as disabled
 	,replaceWithMean   ///< replace attribute value by its mean value
};

//---------------------------------------------------------------------
/// A dataset, holds a set of \ref DataPoint
//template<typename T>
class DataSet
{
	public:
		DataSet() : _nbAttribs(0)
		{}
		DataSet( size_t n ) : _nbAttribs(n)
		{ assert( n ); }

		size_t size() const
		{ return _data.size(); }

		size_t nbAttribs() const
		{ return _nbAttribs; }

		void setNbAttribs( uint n )
		{
			assert( n>1 );
			if( size() )
				throw std::runtime_error( "cannot set size if data set not empty" );
			_nbAttribs = n;
		}

		std::vector<DataPoint>::const_iterator
		begin() const
		{
			return _data.begin();
		}
		std::vector<DataPoint>::const_iterator
		end() const
		{
			return _data.end();
		}

//		template<typename U>
		void addPoint( const DataPoint& dp )
		{
#ifdef DTCPP_ERRORS_ASSERT
			assert( dp.nbAttribs() == _nbAttribs );
#else
			if( dp.nbAttribs() != _nbAttribs )
				throw std::runtime_error(
					"nb attrib: point=" + std::to_string( dp.nbAttribs() )
					+ " dataset=" + std::to_string( _nbAttribs )
				);
#endif // DTCPP_ERRORS_ASSERT
			_data.push_back( dp );

			if( dp.classVal().get() >= 0 )
				_classCount[dp.classVal()]++;
			else
				_nbNoClassPoints++;
			_noChange = false;
		}
//		template<typename U>
		DataPoint& getDataPoint( size_t idx )
		{
#ifdef DTCPP_ERRORS_ASSERT
			assert( idx < _data.size() );
#else
			if( idx >= _data.size() )
				throw std::runtime_error(
					"idx=" + std::to_string( idx )
					+ " dataset size=" + std::to_string( _data.size() )
				);

#endif // DTCPP_ERRORS_ASSERT

			return _data[idx];
		}
//		template<typename U>
		const DataPoint& getDataPoint( size_t idx ) const
		{
			assert( idx < _data.size() );
			return _data[idx];
		}
		bool load( std::string fname, const Fparams=Fparams() );
		void print( std::ostream& ) const;
		void print( std::ostream&, const std::vector<uint>& ) const;
		void printInfo( std::ostream&, const char* name=0 ) const;
		void generateClassDistrib( std::string fname ) const;

		template<typename T>
		void generateAttribPlot( std::string fname, const DatasetStats<T>& ) const;

		void clear()
		{
			_data.clear();
			_classCount.clear();
			_nbNoClassPoints = 0u;
			clearOutliers();
			_noChange = false;
		}
		std::pair<DataSet,DataSet> getFolds( uint i, uint nbFolds ) const;

/// Shuffle the data (taken from https://stackoverflow.com/a/6926473/193789)
		void shuffle()
		{
			std::shuffle(std::begin(_data), std::end(_data), std::random_device() );
		}
		template<typename T>
		DatasetStats<T> computeStats( uint nbBins ) const;
/// \name Outlier handling
///@{
		template<typename T>
		void tagOutliers( const DatasetStats<T>&, En_OD_method odm=En_OD_method::fixedSigma, En_OR_method orm=En_OR_method::disablePoint, float param=3.f );

		bool pointIsOutlier( size_t i ) const
		{
			if( _vIsOutlier.size() )
			{
				assert( i < _vIsOutlier.size() );
				return _vIsOutlier[i];
			}
			return false;
		}
		void clearOutliers()
		{
			_vIsOutlier.clear();
			_nbOutliers = 0;
		}
		size_t nbOutliers() const
		{
			return _nbOutliers;
		}
///@}
		size_t nbClasses( const std::vector<uint>& ) const;

/// Returns nb of classes in the dataset, \b NOT considering the points without any class assigned
/// \todo implement recounting if outliers
		size_t nbClasses() const
		{
			return _classCount.size();
		}

		ClassIndexMap getClassIndexMap() const
		{
			ClassIndexMap cim;
			size_t i = 0;
			for( const auto& cc: _classCount )  // for each class value, fill
				cim[cc.first] = i++;            // the map with an incremental index
			return cim;
		}
		template<typename HISTO>
		std::vector<std::pair<uint,uint>> countClassPerBin( size_t, const HISTO& ) const;

/// Returns the number of points with class \c val (or number of non-assigned points if \c val=-1)
		size_t getClassCount( ClassVal val ) const
		{
			if( val == ClassVal(-1) )
				return _nbNoClassPoints;
			if( _classCount.count( val ) )    // we test first, because it might not be present
				return _classCount.at(val);
			return 0u;
		}

	private:
		void p_countClasses();

	private:
		size_t                  _nbAttribs = 0;
		std::vector<DataPoint>  _data;
		std::map<ClassVal,uint> _classCount;           ///< Holds the number of points for each class value. Does \b NOT count classless points
		uint                    _nbNoClassPoints = 0u;
		std::vector<bool>       _vIsOutlier;            ///< Will be allocated ONLY if tagOutliers() is called, with En_OR_method::disablePoint
		size_t                  _nbOutliers;   ///< to avoid recounting them when unneeded
		std::string             _fname;         ///< file name (saved so it can be printed out in output files)
		bool                    _noChange = false;
};
//using DataSetf = DataSet<float>;
//using DataSetd = DataSet<double>;


//---------------------------------------------------------------------
/// Writes in current folder a file named <code>attrib_histo_<i>.dat</code>, holding
/// the histogram values of attribute \c i. Helper function for DataSet::computeStats()
/**
<br>
- Uses Boost::histogram, see https://www.boost.org/doc/libs/1_70_0/libs/histogram
*/
template<typename T>
auto
genAttribHisto(
	size_t                    atIdx,       ///< attribute index
	const std::vector<float>& vat,
	const AttribStats<T>&     atstats,
	uint                      nbBins,      ///< nb bins of the histogram
	std::string               data_fn      ///< input datafile name
)
{
	char sep = ' ';
	auto h = boost::histogram::make_histogram(
		boost::histogram::axis::regular<>(
			nbBins,
			atstats._minVal,
			atstats._maxVal
		)
	);
	std::string fname( "attrib_histo_" + std::to_string(atIdx) );
	auto f = priv::openOutputFile( fname, priv::FT_DAT, data_fn );

	std::for_each( vat.begin(), vat.end(), std::ref(h) );

	f << "# histogram for attribute " << atIdx << '\n';
	for (auto x : indexed(h) )
		f << x.index()+1 << sep << x.bin().lower() << sep << x.bin().upper() << sep << *x << '\n';
	return h;
}
//---------------------------------------------------------------------
/// Compute statistics of an attribute.
/**
- median: https://stackoverflow.com/a/42791986/193789
- stddev: https://stackoverflow.com/a/7616783/193789

\note Argument must not be const because it will be (partially) sorted here
*/
template<typename T>
AttribStats<T>
computeAttribStats( std::vector<float>& vat )
{
	auto nbPts = vat.size();

	auto it_mm = std::minmax_element( vat.begin(), vat.end() );
	AttribStats<T> at_stat { *it_mm.first, *it_mm.second };     // sets min and max values

	auto sum = std::accumulate( vat.begin(), vat.end(), 0. );
	auto mean = sum / nbPts;
	at_stat._meanVal = mean;

	std::vector<double> diff( nbPts );
	std::transform( vat.begin(), vat.end(), diff.begin(), [mean](double x) { return x - mean; });

	auto sq_sum = std::inner_product( diff.begin(), diff.end(), diff.begin(), 0. );

	at_stat._stddevVal = std::sqrt( sq_sum / nbPts );

	if( nbPts % 2 == 0)  // if even
	{
		const auto median_it1 = vat.begin() + nbPts / 2 - 1;
		const auto median_it2 = vat.begin() + nbPts / 2;

		std::nth_element( vat.begin(), median_it1 , vat.end() );
		const auto e1 = *median_it1;

		std::nth_element( vat.begin(), median_it2 , vat.end() );
		const auto e2 = *median_it2;

		at_stat._medianVal = (e1 + e2) / 2;

	}
	else                // if odd
	{
		const auto median_it = vat.begin() + vat.size() / 2;
		std::nth_element( vat.begin(), median_it , vat.end() );
		at_stat._medianVal = *median_it;
	}
	return at_stat;
}
//---------------------------------------------------------------------
/// For each bin of the histogram \c histo: count the number of classes in the dataset, for attribute \c attrIdx
/**
\return A vector of size equal to the number of bins, holding the number of classes in that bin

\todo check if not problem here: \c histo has 2 additional bins (first and last, for values higher and lower).
Isn't that a problem ?
*/
template<typename HISTO>
std::vector<std::pair<uint,uint>>
DataSet::countClassPerBin( size_t attrIdx, const HISTO& histo ) const
{
	auto nbBins = histo.size();
	std::vector<std::set<ClassVal>> classSets( nbBins ); // one set of classes per bin

	for(size_t idx=0; idx<size(); idx++ )
	{
		const auto& pt = getDataPoint(idx);        // for each point
		auto attribVal = pt.attribVal( attrIdx );  // get attribute value
		auto classVal = pt.classVal();             // and class value

		if( classVal != ClassVal(-1)               // if not classless, then
			&& !pointIsOutlier(idx) )              // AND not an outlier
		{                                          // then assign it to the correct bin
			size_t i = 0;
			for (auto&& x : boost::histogram::indexed(histo) )
			{
				if( attribVal > x.bin().lower() && attribVal <= x.bin().upper() )
					classSets[i].insert( classVal );
				i++;
			}
		}
	}

	std::vector<std::pair<uint,uint>> v_ret( nbBins );
	size_t i = 0;
	for (auto&& x : boost::histogram::indexed(histo) )
	{
		v_ret[i].first  = classSets[i].size();
		v_ret[i].second = *x;
		i++;
	}

	return v_ret;
}
//---------------------------------------------------------------------
/// Saves in current folder the histogram of nb of classes per bin, for attribute \c attrIdx.
/// Related: DataSet::countClassPerBin()
void
saveClassCountPerBin( size_t attrIdx, const std::vector<std::pair<uint,uint>>& v_ccpb )
{
	char sep = ' ';

	auto f = priv::openOutputFile( "histo_ccpb_attrib_" + std::to_string(attrIdx), priv::FT_DAT );
	assert( f.is_open() );
	f << "# attribute " << std::to_string(attrIdx)
		<< "\n# index "
		<< sep << "nb_of_classes_per_bin"
		<< sep << "ratio_nb_classes/nb_pts_per_bin\n";
	assert( v_ccpb.size()>3 );
/*
We start at 1 and stop before the last one, because this is build from the
Boost::histogram object, and that thing always two additional bins, the first for
values lower than the "low" threshold, and one for values above the "high" threshold
*/
	for( size_t i=1; i<v_ccpb.size()-1; i++ )
	{
		f << i << sep << v_ccpb[i].first << sep;
		if( v_ccpb[i].second  )
			f << 1.0 * v_ccpb[i].first / v_ccpb[i].second;
		else
			f << '0';
		f << '\n';
	}
}
//---------------------------------------------------------------------
/// Outlier detection for an attribute, returns true if it is detected as so.
/// Helper function for DataSet::tagOutliers()
template<typename T>
bool
attribIsOutlier( T atval, AttribStats<T> stat, En_OD_method odm, float param )
{
	switch( odm )
	{
		case En_OD_method::fixedSigma:
			if(
				atval > stat._meanVal + param * stat._stddevVal
			||
				atval < stat._meanVal - param * stat._stddevVal
			)
				return true;
		break;

		case En_OD_method::ChauvenetCrit:
		break;

		default: assert(0);
	}
	return false; // TEMP
}
//---------------------------------------------------------------------
/// Called after tagging outliers, because some classes might have vanished.
void
DataSet::p_countClasses()
{
	if( nbOutliers() == 0 && _noChange )  // then, no changes
		return;

	_nbNoClassPoints = 0;
	_classCount.clear();
	for( size_t p=0; p<size(); p++ )
	{
		const auto& pt = getDataPoint(p);
		if( !pointIsOutlier(p) )
		{
			if( pt.classVal() == ClassVal(-1) )
				_nbNoClassPoints++;
			else
				_classCount[ pt.classVal() ]++;
		}
	}
	_noChange = true;
}
//---------------------------------------------------------------------
/// Search and tag for outliers in the dataset. The exact action taken depends on \c orm
/**
- if orm=replaceWithMean, then the outlier attribute value will have its value replaced by the mean value of the attribute
- if orm=disablePoint, then the point will simply be tagged as outlier, thus not taken into account when training

Default behavior is to discard dataset points that have an attribute more than 2 sigma away from mean value.
*/
template<typename T>
void
DataSet::tagOutliers( const DatasetStats<T>& stats, En_OD_method odm, En_OR_method orm, float param )
{
	_nbOutliers = 0;
	if( orm == En_OR_method::disablePoint )
	{
		_vIsOutlier.resize( size() );
		std::fill( _vIsOutlier.begin(), _vIsOutlier.end(), false );
	}
	for( size_t p=0; p<size(); p++ )
	{
		bool ptDisabled = false;
		auto& pt = getDataPoint(p);
		for( size_t i=0; i<nbAttribs() && !ptDisabled; i++ )  // loop through all attributes
		{                                                           // but stop if point is already disabled
			auto& atval = pt.attribVal(i);
			if( attribIsOutlier( atval, stats.get(i), odm, param ) )
			{
				_nbOutliers++;
				switch( orm )
				{
					case En_OR_method::disablePoint:
						_vIsOutlier.at(p) = true;
						ptDisabled = true;
					break;
					case En_OR_method::replaceWithMean:
						atval = stats.get(i)._meanVal;
					break;
					default: assert(0);
				}
			}
		}
	}
	p_countClasses();
}

//---------------------------------------------------------------------
/// Compute statistics of the dataset, attribute by attribute, and saves histogram in data files.
/// Also generates a Gnuplot script to plot these.
/**
Done by storing for a given attribute all the values in a vector, then computing stats on that vector
*/
template<typename T>
DatasetStats<T>
DataSet::computeStats( uint nbBins ) const
{
	START;
	auto fplot = priv::openOutputFile( "plot_attrib_histo", priv::FT_PLT, _fname );
	fplot << priv::g_root_gnuplot_histo << "\n";

	DatasetStats<T> dstats( nbAttribs() );
	for( size_t atItx=0; atItx<nbAttribs(); atItx++ )
	{
		std::vector<float> vat;
		vat.reserve( size() );               // guarantees we won't have any reallocating
		if( nbOutliers() == 0 )
			for( const auto& point: _data )
				vat.push_back( point.attribVal(atItx) );
		else
			for( size_t ptIdx=0; ptIdx<size(); ptIdx++ )
			{
				const auto& point = getDataPoint(ptIdx);
				if( !pointIsOutlier(ptIdx) )
					vat.push_back( point.attribVal(atItx) );
			}

		const auto& atstats = computeAttribStats<T>( vat );
		dstats.add( atItx, atstats );

		auto histo = genAttribHisto( atItx, vat, atstats, nbBins, _fname );

		auto v_ccpb = countClassPerBin( atItx, histo );
		saveClassCountPerBin( atItx, v_ccpb );

		fplot << "set output 'attrib_histo_"<< atItx << ".png'\n"
			<< "set multiplot\n"
			<< "set logscale y\n"
			<< "set title 'Nb pts per bin'\n"
			<< "set origin 0,0\n"
			<< "set size 1,0.5\n"
			<< "plot 'attrib_histo_" << atItx << ".dat' using 4:xtic(1) noti\n"
			<< "set title 'Ratio Nb classes per bin/nbpts'\n"
			<< "set origin 0,0.5\n"
			<< "set size 1,0.5\n"
			<< "plot 'histo_ccpb_attrib_" << atItx << ".dat' using 3:xtic(1) noti\n"
			<< "unset multiplot\n"
			<< '\n';
	}

	return dstats;
}
//---------------------------------------------------------------------
/// Returns nb of classes in the subset given by the indexes in \c vIdx
size_t
DataSet::nbClasses( const std::vector<uint>& vIdx ) const
{
	std::set<ClassVal> classSet;
	for( const auto idx: vIdx )
	{
		const auto& pt = getDataPoint( idx );
		if( pt.classVal() != ClassVal(-1) )
			classSet.insert( pt.classVal() );
	}
	return classSet.size();
}
//---------------------------------------------------------------------
/// Returns a pair of two subsets of the data, first is the training data, second is the test data
/**
If 100 pts and nbFolds=5, this will return 20 pts in \c ds_test and 80 pts in \c ds_train

If the \f$ nbPts/nbFolds \f$  is not an integer value, then the test set will hold \f$ nbPts/nbFolds \f$ points
and the training set will hold the rest of the points.

The \c index defines which fraction of the points are returned in the test set

\todo Remove outliers from folds !
*/
std::pair<DataSet,DataSet>
DataSet::getFolds( uint index, uint nbFolds ) const
{
 	DataSet ds_train( nbAttribs() );
 	DataSet ds_test(  nbAttribs() );
	uint nb = size() / nbFolds;
	for( uint i=0; i<size(); i++ )
	{
		if( i / nb == index )
			ds_test.addPoint( getDataPoint(i) );
		else
			ds_train.addPoint( getDataPoint(i) );
	}

	COUT << "ds_test #=" << ds_test.size()
		<< " ds_train #=" << ds_train.size() << "\n";

	return std::make_pair( ds_train, ds_test );
}

//---------------------------------------------------------------------
// % % % % % % % % % % % % % %
namespace priv {
// % % % % % % % % % % % % % %

/// Helper function for DataSet::generateAttribPlot()
void addVerticalLine( std::ostream& f, std::string label, float vpos, float xpos, std::string color )
{
	f << "set arrow from " << xpos << ", graph 0 to " << xpos << ", graph 1 nohead lc rgb '"
		<< color << "' lw 1\n"
		<< "set label '" << label << "' at " << xpos << ", graph " << vpos << " rotate by 90 front textcolor rgb '"
		<< color << "'\n";
}
// % % % % % % % % % % % % % %
} // namespace priv
// % % % % % % % % % % % % % %

//---------------------------------------------------------------------
/// Generates two files in current folder: 1-a Gnuplot script, to plot data, and
/// 2- the whole dataset in a csv file, so that it has always the same format
/**
\note You could of course write a plotting script yourself to plot
the input data file, the code here just abstracts the input file format details,
and saves you the burden of writing such a script.<br>
Moreover, you can always tweak the generated script to fit your needs.
*/
template<typename T>
void
DataSet::generateAttribPlot(
	std::string fname, ///< File name, no extension (the 2 files will have that name, with extensions .plt and .csv)
	const DatasetStats<T>& dss
) const
{
	START;
	auto f1 = priv::openOutputFile( fname, priv::FT_CSV, _fname );
	if( _vIsOutlier.size() )
	{
		for( size_t i=0; i<size(); i++ )
			if( !_vIsOutlier[i] )
				getDataPoint(i).print( f1 );
	}
	else
		for( const auto& pt: _data )
			pt.print( f1 );
	f1 << '\n';

	auto f = priv::openOutputFile( fname, priv::FT_PLT, _fname );
	f << "set ylabel 'CLASS'"
		<< "\nset yrange [-0.5:" << nbClasses()-0.5f << ']'
		<< "\nclass=" << nbAttribs()+1
		<< "\nset datafile separator ' '"
		<< "\nset grid"
		<< "\n\n";

	for( size_t i=0; i<nbAttribs(); i++ )
	{
		auto st = dss.get(i);
		f << "set output '" << fname << '_' << i << ".png'\n"
			<< "unset arrow\n"
			<< "unset label\n";
		priv::addVerticalLine( f, "mean",       0.8, st._meanVal,               "red" );
		priv::addVerticalLine( f, "mean-sigma", 0.7, st._meanVal-st._stddevVal, "blue" );
		priv::addVerticalLine( f, "mean+sigma", 0.7, st._meanVal+st._stddevVal, "blue" );
		priv::addVerticalLine( f, "median",     0.6, st._medianVal,             "green" );
		f << "set title 'Class vs. attribute " << i << "'\n"
			<< "set ytics 0,1," << nbClasses()-1
			<< "\nplot '" << fname << ".csv' using " << i+1 << ":class notitle\n"
			<< '\n';
	}
}

//---------------------------------------------------------------------
//template<typename T>
bool
DataSet::load( std::string fname, const Fparams params )
{
	std::ifstream f( fname );
	if( !f.is_open() )
	{
		std::cerr << "Unable to open file " << fname << "\n";
		return false;
	}
	_fname = fname;
	clear();

	std::map<std::string,uint> classStringMap;  // maps string to class Idx used only if classes are given as strings
	uint classIndexCounter = 0;

	size_t nb_lines     = 0;
	size_t nb_empty     = 0;
	size_t nb_comment   = 0;
	do
	{
		std::string temp;
		std::getline( f, temp );
		nb_lines++;

		if( temp.empty() )          // if empty
			nb_empty++;
		else                        // if NOT empty
		{
			if( temp.at(0) == '#' )  // if comment
				nb_comment++;
			else                     // if NOT comment
			{
				auto v_tok = priv::splitString( temp, params.sep );
				if( v_tok.size() < 2 )
				{
					std::cerr << "-Error: only one value on line " << nb_lines
						<< "\n-Line=" << temp << " \n-length=" << temp.size() << '\n';
					return false;
				}

				if( size() == 0 )                    // if this is the first datapoint, then set the nb of attributes
					setNbAttribs( params.dataFilesHoldsClass ? v_tok.size()-1 : v_tok.size() );

				if( !params.dataFilesHoldsClass )
					_data.push_back( DataPoint( v_tok ) );
				else
				{
					int classIndex = -1;
					auto cla = v_tok.back();
					if( params.classIsfirst )
						cla = v_tok.front();

					if( !params.classAsString )
					{
						try
						{
							classIndex = std::stoi( cla );
						}
						catch( ... )
						{
							throw std::runtime_error( "Unable to convert string '" + cla + "' on line " + std::to_string(nb_lines) + " to an integer value" );
						}
					}
					else
					{
						if( classStringMap.find( cla ) == classStringMap.end() )  // if not registered, then
						{
							classIndex            = classIndexCounter;
							classStringMap[ cla ] = classIndexCounter;             // new class, add it
							classIndexCounter++;
						}
						else
							classIndex = classStringMap[ cla ];
					}
					if( classIndex < 0 )
						_nbNoClassPoints++;

					else
						_classCount[ ClassVal(classIndex) ]++;

					if( params.classIsfirst )
						std::rotate( v_tok.begin(), v_tok.begin()+1, v_tok.end() );
					v_tok.erase( v_tok.end()-1 );   // remove last element (class)
					_data.push_back( DataPoint( v_tok, ClassVal(classIndex) ) );
				}
			}
		}
	}
	while( !f.eof() );

#if 1
	std::cout << " - Read " << size() << " points in file " << fname;
	std::cout << "\n - file info:"
		<< "\n  - nb lines=" << nb_lines
		<< "\n  - nb empty=" << nb_empty
		<< "\n  - nb comment=" << nb_comment
//		<< "\n  - nb classes=" << classValues.size()
		<< "\n  - nb classes=" << nbClasses()
		<< '\n';
#endif
	return true;
}
//---------------------------------------------------------------------
void
DataSet::generateClassDistrib( std::string fname ) const
{
	START;
	auto fhisto = priv::openOutputFile( fname, priv::FT_DAT, _fname );

	fhisto << "# data class histogram file for input file '" <<  fname
		<< "'\n# class_index occurence_count percentage\n";
	fhisto << "-1 " << getClassCount( ClassVal(-1) ) << ' '
		<< 100. * getClassCount( ClassVal(-1) ) / size() << '\n';
	for( const auto& cval: _classCount )
		fhisto << cval.first << " "
			<< cval.second << " " << 100. * cval.second/size()
			<< '\n';

	auto fplot = priv::openOutputFile( fname, priv::FT_PLT, _fname );
	fplot << "set output '" << fname << ".png'\n"
		<< "set title 'Class distribution'\n"
		<< "set ylabel '% of total points'\n"
		<< "set xlabel 'Class'\n"
		<< "set style data histogram\n"
		<< "set style histogram cluster gap 1\n"
		<< "set style fill solid\n"
		<< "set boxwidth 1\n"
		<< "plot '" << fname << ".dat' using 3:xtic(1) notitle\n";
}

//---------------------------------------------------------------------
void
DataSet::printInfo( std::ostream& f, const char* name ) const
{
	f << "Dataset: ";
	if( name )
		f << name;
	f << "\n # points="             << size()
		<< "\n # attributes="       << nbAttribs()
		<< "\n # classes="          << nbClasses()
		<< "\n # classless points=" << _nbNoClassPoints;

//	if( _vIsOutlier.size() )
		f << "\n # outliers=" << _nbOutliers;

	f << "\nClasses frequency:\n";
	size_t sum = 0;
	for( const auto& cval: _classCount )
	{
		std::cout << cval.first << ": "
			<< cval.second
			<< " (" << std::setw(4) << 100. * cval.second/size()
			<< " %)\n";
		sum += cval.second;
	}
	f << " => " << sum << " points holding a class value\n";
}
//---------------------------------------------------------------------
//template<typename T>
void
DataSet::print( std::ostream& f ) const
{
	f << "# -------------------------------------------\n";
	f << "# Dataset, nb pts=" << size() << " nb attributes=" << nbAttribs() << "\n";
	for( size_t i=0; i<nbAttribs(); i++ )
		f << i << "; ";
	f << " class\n";

	for( const auto& pt: _data )
		f << pt;
	f << "# -------------------------------------------\n";
}
//---------------------------------------------------------------------
//template<typename T>
void
DataSet::print( std::ostream& f, const std::vector<uint>& vIdx ) const
{
	f << "# -------------------------------------------\n";
	f << "# Dataset, total nb pts=" << size()
		<< " requested=" << vIdx.size()
		<< " nb attributes=" << nbAttribs() << "\n";
	for( size_t i=0; i<nbAttribs(); i++ )
		f << i << "; ";
	f << " class\n";
	for( const auto& id: vIdx )
	{
		const auto& pt = getDataPoint( id );
		f << id << " ";
		for( const auto& val: pt._attrValue )
			f << val << ";";

		f << pt.classVal() << "\n";
	}
	f << "# -------------------------------------------\n";
}

//---------------------------------------------------------------------
/// Holds the node type
enum NodeType
{
	 NT_undef = 0
	 ,NT_Root
	 ,NT_Decision
	 ,NT_Final_MD
	 ,NT_Final_GI_Small
	 ,NT_Final_SplitTooSmall
	 ,NT_Merged
};

inline
std::string
getString( NodeType nt )
{
	const char* s = nullptr;
	switch( nt )
	{
		case NT_undef:    s="UNDEF";    break;
		case NT_Root:     s="Root";     break;
		case NT_Decision: s="Decision"; break;
//		case NT_Final:    s="Final";    break;
		case NT_Final_MD:            s="MD";  break;
		case NT_Final_GI_Small:      s="MGI"; break;
		case NT_Final_SplitTooSmall: s="STS"; break;
		case NT_Merged:              s="ME";  break;
		default: assert(0);
	}
	return std::string(s);
}
//---------------------------------------------------------------------
/// A node of the training tree
struct NodeT
{
	static uint s_Counter;           ///< Node counter, incremented at each node creation, reset with resetNodeId()
	uint     _nodeId = 0;            ///< Id of the node. Needed to print the dot file. \todo could be removed if graph switches to \c VecS
	NodeType _type = NT_undef;       ///< Type of the node (Root, leaf, or decision)
	ClassVal _class = ClassVal(-1);  ///< Class (only for terminal nodes)
	size_t   _attrIndex = 0;         ///< Attribute Index that this nodes classifies
	float    _threshold = 0.f;       ///< Threshold on the attribute value (only for decision nodes)
	uint     depth = 0;              ///< Depth of the node in the tree
	float    giniImpurity = 0.f;
	std::vector<uint> v_Idx;         ///< Data point indexes

	friend std::ostream& operator << ( std::ostream& f, const NodeT& n )
	{
		f << "C=" << n._class
			<< "\nattr=" << n._attrIndex
			<< "\nthres=" << n._threshold
			<< "\ndepth=" << n.depth
			<< "\n#v=" << n.v_Idx.size()
			;
		return f;
	}

	bool isLeave() const
	{
		assert( _type != NT_undef );
		if( _type == NT_Root )
			return false;
		if( _type == NT_Decision )
			return false;
		return true;
	}

/// Reset of node counter \ref s_Counter
	static void resetNodeId()
	{
		s_Counter = 0;
	}
	NodeT() { _nodeId = s_Counter++; }
};

 /// Instanciation of static
 uint NodeT::s_Counter = 0;

//---------------------------------------------------------------------
/// Used for training
/**
\note IMPORTANT: we use list because when splitting the vector of indexes of points of a node,
we use the current node's vector as a reference (to avoid copying the whole vector).
BUT: if we where using \c vectS, adding new nodes may invalidate the current vertex descriptor.
Thus, we use \c listS
\todo: actually, maybe we need to use ListS only for ONE of the two containers. Check BGL doc
to see what these two template parameters are used for.

\note 2021-03-01: changed from \c directedS to \c bidirectionalS: required to be able to get in_edges (see TraingTree::pruning())
*/
using GraphT = boost::adjacency_list<
		boost::listS,  // boost::vecS,
		boost::listS,
		boost::bidirectionalS, //boost::directedS,
		NodeT,
		priv::EdgeData
	>;

using vertexT_t = boost::graph_traits<GraphT>::vertex_descriptor;
using edge_t = boost::graph_traits<GraphT>::edge_descriptor;

//---------------------------------------------------------------------
// forward declaration, needed for the friend declaration below
class ConfusionMatrix;

// % % % % % % % % % % % % % %
namespace priv {
// % % % % % % % % % % % % % %

/// Private class, used to hold the counters extracted from the ConfusionMatrix,
/// see ConfusionMatrix::p_score()
class CM_Counters
{
	friend class dtcpp::ConfusionMatrix;

	CM_Counters( double TP, double FP, double TN, double FN )
		: tp(TP),fp(FP),tn(TN), fn(FN)
	{
		assert( tp + fp + tn + fn > 0 );
	}
	double tp,fp,tn,fn;
};
// % % % % % % % % % % % % % %
} // namespace priv
// % % % % % % % % % % % % % %

//---------------------------------------------------------------------
/// Performance score of classification, see ConfusionMatrix for definitions
enum CM_Score
{
	CM_TPR=0    ///< True Positive Rate
	,CM_TNR     ///< True Negative Rate
	,CM_ACC     ///< Accuracy
	,CM_BACC    ///< Balanced Accuracy
	,CM_F1      ///< F1-score, see https://en.wikipedia.org/wiki/F-score

	,CM_SCORE_END  ///< only used to iterate
};

//---------------------------------------------------------------------
std::string
getString( CM_Score n )
{
	const char* s = nullptr;
	switch( n )
	{
		case CM_TPR:  s="TPR";  break;
		case CM_TNR:  s="TNR";  break;
		case CM_ACC:  s="ACC";  break;
		case CM_BACC: s="BACC"; break;
		case CM_F1:   s="F1";   break;
		default: assert(0);
	}
	return std::string(s);
}

//---------------------------------------------------------------------
/// Container for the performance scores
struct Scores
{
	std::array<double, CM_SCORE_END> _sScoreValues;
};

//---------------------------------------------------------------------
/// Confusion Matrix, handles both 2 class and multiclass problems, but usage will be different
/**

Instanciated in TrainingTree::classify()

- Layout:
 - columns: true class
 - lignes: predicted (classified) class

- To get the associated performance scores, use getScore() (2 versions).

- Definitions of performance scores:
 - For 2- class problems, follows definitions from https://en.wikipedia.org/wiki/Confusion_matrix
 - For multiclass, see definitions here: https://stats.stackexchange.com/a/338240/23990
and here: https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872

Usage:

For 2-class problem, you get (for example) the "True Positive Rate" metric like this:
\code
	auto score = cmat.getScore( CM_TPR );
\endcode

For multiclass situations, you need to add for what class you are requesting this.
\code
	auto score = cmat.getScore( CM_TPR, ClassValue );
\endcode

*/
struct ConfusionMatrix
{
	friend std::ostream& operator << ( std::ostream&, const ConfusionMatrix& );

	ConfusionMatrix( ClassIndexMap cim )
		: _cmClassIndexMap(cim)
	{
		auto nbClasses = cim.size();
		assert( nbClasses>1 );
		_mat.resize( nbClasses );
		for( auto& li: _mat )
		{
			li.resize( nbClasses );
			std::fill( li.begin(), li.end(), 0u );
		}
	}

#ifdef TESTMODE
	ConfusionMatrix( size_t nbClasses )
	{
		assert( nbClasses>1 );
		_mat.resize( nbClasses );
		uint i = 0;
		for( auto& li: _mat )
		{
			li.resize( nbClasses );
			std::fill( li.begin(), li.end(), 0u );
			_cmClassIndexMap[ ClassVal(i) ] = i;
			i++;
		}
	}

	ConfusionMatrix( const std::vector<std::vector<uint>>& m )
	{
		_mat = m;
	}
#endif //  TESTMODE

	void clear()
	{
		for( auto& li: _mat )
			std::fill( li.begin(), li.end(), 0u );
	}
	size_t nbClasses() const
	{
        return _mat.size();
	}
/*	void assignIndexMap( const ClassIndexMap& cim )
	{
		assert( cim.size() == nbClasses() );
		_cmClassIndexMap = cim;
	}*/
    double getScore( CM_Score, ClassVal ) const;
    double getScore( CM_Score ) const;

/// Returns total number of values in matrix
	size_t nbValues() const
	{
		size_t sum = 0u;
		for( const auto& li: _mat )
			sum += std::accumulate( li.begin(), li.end(), 0u );
		return sum;
	}

/// Returns number of \b predicted values for class \c cval
	size_t nbValues( ClassVal cval ) const
	{
		assert( cval.get() >= 0 );
		auto li  = static_cast<size_t>( cval.get() );
		assert( li < _mat.size() );
		return std::accumulate( _mat[li].begin(), _mat[li].end(), 0u );
	}

	void add( ClassVal trueVal, ClassVal predictedVal )
	{
		assert( trueVal.get() >= 0 );
		assert( predictedVal.get() >=0 );

		auto col = static_cast<size_t>( trueVal.get() );
		auto li  = static_cast<size_t>( predictedVal.get() );
		if( _cmClassIndexMap.size() )  /// \todo this is now always true
		{
			col = _cmClassIndexMap.at( trueVal );
			li  = _cmClassIndexMap.at( predictedVal );
		}

		assert( li < _mat.size() && col < _mat.size() );
		_mat[li][col]++;
	}
	void printAllScores( std::ostream&, const char* msg=0 ) const;

	private:
		double p_score( CM_Score scoreId, priv::CM_Counters ) const;

	private:
		std::vector<std::vector<uint>> _mat;
		ClassIndexMap                  _cmClassIndexMap;
};

//---------------------------------------------------------------------
// % % % % % % % % % % % % % %
namespace priv {
// % % % % % % % % % % % % % %
void printLine( std::ostream& f, uint w, uint n )
{
	n++;
	f << '|';
	for( uint i=0; i<w*n+5; i++ )
		f << '-';
	f <<"|";
}
// % % % % % % % % % % % % % %
} // namespace priv
// % % % % % % % % % % % % % %
//---------------------------------------------------------------------
/// Streaming of \ref ConfusionMatrix
std::ostream&
operator << ( std::ostream& f, const ConfusionMatrix& cm )
{
	auto nbVal = cm.nbValues();
	size_t w = std::max( (size_t)3, (size_t)std::to_string( nbVal ).size() );
	auto nbCl = cm._mat.size();
	f << std::setfill(' ');
	f << "ConfusionMatrix:\nPredicted \\ True class =>\n ||   ";

	for( const auto & pci: cm._cmClassIndexMap )
		f << std::setw(w) << pci.first << " ";

	f << "  class\n \\/ ";
	priv::printLine( f, w, nbCl );
	f << "  # | rate\n";

	std::vector<size_t> sumCol( nbCl, 0u );
	size_t li = 0;
	for( const auto & pci: cm._cmClassIndexMap )
	{
		f << std::setw(3) << pci.first << " | ";

		for( size_t col=0; col<nbCl; col++ )
		{
			f << std::setw(w) << cm._mat[li][col] << ' ';
			sumCol[col] += cm._mat[li][col];
		}
		f << "| "
			<< std::setw(w) << cm.nbValues( ClassVal(li) ) << "  "
			<< std::setprecision(3)
			<< 100.0 * cm.nbValues( ClassVal(li) ) / nbVal
			<< "%\n";
		li++;
	}
	f << "    ";
	priv::printLine( f, w, nbCl );
	f << "\nsum | ";
	for( size_t col=0; col<nbCl; col++ )
		f << std::setw(w) << sumCol[col] << ' ';
	f <<  "| " << std::setw(w)
		<< std::accumulate( sumCol.begin(), sumCol.end(), 0u )
		<< '\n';
	return f;
}

//---------------------------------------------------------------------
/// Private, compute scores for both 2-class and multi-class confusion matrices
double
ConfusionMatrix::p_score( CM_Score scoreId, priv::CM_Counters cmc ) const
{
	auto TPR = cmc.tp / ( cmc.tp + cmc.fn );
	auto TNR = cmc.tn / ( cmc.tn + cmc.fp );
	double scoreVal = 0.;
	switch( scoreId )
	{
		case CM_TPR:  scoreVal =  TPR; break;
		case CM_TNR:  scoreVal =  TNR; break;
		case CM_ACC:  scoreVal = (cmc.tp + cmc.tn)/nbValues(); break;
		case CM_BACC: scoreVal = (TPR + TNR ) / 2.; break;
		case CM_F1:   scoreVal = 2.* cmc.tp / ( 2.* cmc.tp + cmc.fp + cmc.fn ); break;
		default: assert(0);
	}
	return scoreVal;
}
//---------------------------------------------------------------------
/// Computes performance scores. Used for 2-class situations
double
ConfusionMatrix::getScore( CM_Score scoreId ) const
{
	assert( nbValues() > 1 );
	assert( nbClasses() == 2 );

	const auto& TP = _mat[0][0];
	const auto& FP = _mat[0][1];
	const auto& FN = _mat[1][0];
	const auto& TN = _mat[1][1];

	return p_score( scoreId, priv::CM_Counters(TP,FP,TN,FN) );
}
//---------------------------------------------------------------------
/// Computes performance scores. Used for multi-class situations
double
ConfusionMatrix::getScore( CM_Score scoreId, ClassVal cval ) const
{
	assert( nbValues() > 2 );
	assert( nbClasses() > 2 );

	assert( cval.get() >= 0 );
	size_t c = static_cast<size_t>( cval.get() );
	assert( c < nbClasses() );

	const auto& TP = _mat[c][c];

	const auto FP = std::accumulate( std::begin(_mat[c]), std::end(_mat[c]), 0. ) - TP;

	auto FN = 0u;
	for( size_t li=0; li<_mat.size(); li++ )
		if( li != c )
			FN += _mat[li][c];

	const auto TN = nbValues() - TP - FN - FP;

	return p_score( scoreId, priv::CM_Counters(TP,FP,TN,FN) );
}
//---------------------------------------------------------------------
/// Prints all the performance scores
/// \todo replace _cmClassIndexMap with a boost::bimap
void
ConfusionMatrix::printAllScores( std::ostream& f, const char* msg ) const
{
	f << "Performance scores";
	if( msg )
		f << " - " << msg;
	f << '\n';
	if( nbClasses() > 2 )
	{
		for( size_t c=0; c<nbClasses(); c++ )
		{
			f << " * class idx=" << c << " label=" << " TODO: use boost::bimap" << ":\n";
			for( auto i=0; i<CM_SCORE_END; i++ )
			{
				auto eni = static_cast<CM_Score>(i);
				f << "   - " << getString( eni ) << " = " << getScore( eni, ClassVal(c) ) << '\n';
			}
		}
	}
	else
	{
		for( auto i=0; i<CM_SCORE_END; i++ )
		{
			auto eni = static_cast<CM_Score>(i);
			f << " - " << getString( eni ) << " = " << getScore( eni ) << '\n';
		}
	}
}
//---------------------------------------------------------------------
/// This one holds edges that each have a vector holding the index of datapoints.
/// This is memory costly, but useless for classifying, so once it is trained, we could use another tree type
/**
Two constructors available, depending on the situation
- the first one requires only the number of classes. However, the assumes the classes will be identified
by their value, i.e. if we have 3 classes, then the \b MUST have the values 0, 1, 2.
If not, then this will cause an error, because access to the \ref ConfusionMatrix will be made using the class
values as indexes. And if the class values are "3", "4", "5", this will trigger an error.
- the second constructor need a \ref ClassIndexMap object, that will enable using the true values of the class to
access the Confusion Matrix. It can be generated from a source dataset with:
\code
	auto classIndexMap = dataset.getClassIndexMap();
\endcode
*/
//template<typename T>
class TrainingTree
{
	private:
		GraphT        _graph;
		vertexT_t     _initialVertex;
		size_t        _maxDepth = 1;  ///< defined by training
		uint          _nbClasses;
		ClassIndexMap _classIndexMap;  ///< maps class values to index values

	public:
/// Constructor 1. Needs to known (at least) the number of classes (so it can define ConfusionMatrix)
/*		TrainingTree( uint nbClasses )
			: _nbClasses( nbClasses )
		{}*/

/// Constructor 2, to be used if class values can not be used as indexes.
		TrainingTree( const ClassIndexMap& cim )
			: _nbClasses( cim.size() )
			, _classIndexMap( cim )
		{}

		void clear()
		{
			_graph.clear();
		}

		void     train( const DataSet&, Params params=Params() );
		ConfusionMatrix classify( const DataSet& ) const;
		ClassVal        classify( const DataPoint& ) const;

		void     printDot( std::string fname ) const;
		void     printInfo( std::ostream&, const char* msg=0 ) const;
		size_t   maxDepth() const { return _maxDepth; }
		size_t   nbLeaves() const;

		size_t pruning();
};

//---------------------------------------------------------------------
/// Iterates on all nodes and counts the one that are not root, nor "decision" nodes
inline
size_t
TrainingTree::nbLeaves() const
{
	size_t c = 0;
	for(
		auto pit = boost::vertices( _graph );
		pit.first != pit.second;
		pit.first++
	)
	{
		auto type = _graph[*pit.first]._type;
		assert( type != NT_undef );
		if( type != NT_Root && type != NT_Decision )
			c++;
	}
	return c;
}


// % % % % % % % % % % % % % %
namespace priv {
// % % % % % % % % % % % % % %

//---------------------------------------------------------------------
/// Recursive function used to print the Dot file, prints the current node
inline
void
printDotNodeChilds( std::ostream& f, vertexT_t vert, const GraphT& graph )
{
	START;
//	COUT << "nb out edges=" << boost::out_degree( v, graph ) << "\n";
//	std::cout.precision(4);
	f.precision(3);
	for(
		auto pit=boost::out_edges(vert, graph);
		pit.first != pit.second;
		pit.first++
	)
	{
		auto target = boost::target( *pit.first, graph );
		COUT << "target node id=" << graph[target]._nodeId << " type=" << (int)graph[target]._type << '\n';
		assert( graph[target]._type != NT_undef );

		f << graph[target]._nodeId
			<< " [label=\"n" << graph[target]._nodeId << ' ';
		if( graph[target]._type == NT_Decision )
			f << "attr=" << graph[target]._attrIndex
				<< " thres=" << graph[target]._threshold << "\\n";
		else
			f << "C" << graph[target]._class
				<< " GI=" << graph[target].giniImpurity
				<< "\\nSR=" << getString( graph[target]._type );

		f //<< "\\ndepth=" << graph[target].depth
			<< " #" << graph[target].v_Idx.size()
			<< "\"";
		switch( graph[target]._type )
		{
			case NT_Decision: f << ",color=green"; break;
			default:          f << ",color=red"; break;
//			default: assert(0);
		}
		f << "];\n";
		f << graph[vert]._nodeId << "->" << graph[target]._nodeId  << ";\n";
		printDotNodeChilds( f, target, graph );
	}
}
// % % % % % % % % % % % % % %
} // namespace priv
// % % % % % % % % % % % % % %

//---------------------------------------------------------------------
/// Print a DOT file of the tree by calling the recursive function \ref printDotNodeChilds()
inline
void
TrainingTree::printDot( std::string fname ) const
{
	auto f = priv::openOutputFile( fname, priv::FT_DOT );
	f << "# file: " << fname << "\n\n"
		<< "digraph g {\nnode [shape=\"box\"];\n"
		<< _graph[_initialVertex]._nodeId
		<< " [label=\"n" << _graph[_initialVertex]._nodeId
		<< " attr="     << _graph[_initialVertex]._attrIndex
		<< " thres="    << _graph[_initialVertex]._threshold
		<< "\\n#"      << _graph[_initialVertex].v_Idx.size()
		<< "\",color = blue];\n";

	f << std::endl; // TEMP
	COUT <<"ID=" << _graph[_initialVertex]._nodeId << " TYPE=" << _graph[_initialVertex]._type << std::endl;

	priv::printDotNodeChilds( f, _initialVertex, _graph );
	f << "}\n";
}

//---------------------------------------------------------------------
/// Print a DOT file of the tree
//template<typename T>
void
TrainingTree::printInfo( std::ostream& f, const char* msg ) const
{
	START;
	f << "Training tree info:";
	if( msg )
		f << msg;
	f << "\n -nb nodes=" << boost::num_vertices( _graph )
		<< "\n -nb edges=" << boost::num_edges( _graph )
		<< "\n -max depth=" << maxDepth()
		<< "\n -nb of leaves=" << nbLeaves()
		<< '\n';
//	f << "Boost printing:\n";
//	boost::print_graph( _graph );  // <= does not work with boost::ListS as container, needs an index attached as dynamic property !
}

//---------------------------------------------------------------------
// deprecated
#if 0
/// Computes the nb of votes for each class, for the points defined in \c v_Idx
/**
\warning This does not take into account class value -1, as it means point is "classless"
*/
std::map<ClassVal,uint>
computeClassVotes( const std::vector<uint>& v_Idx, const DataSet& data )
{
	START;
	assert( v_Idx.size()>0 );
	size_t nbClassLess = 0;
	std::map<ClassVal,uint> classVotes; // key: class index, value: votes
	for( auto idx: v_Idx )
	{
		const auto& dp = data.getDataPoint( idx );
		if( dp.classVal() == ClassVal(-1) )
			nbClassLess++;
		else
			classVotes[ dp.classVal() ]++;
	}
	return classVotes;
}
#endif
//---------------------------------------------------------------------
/// Computes the Gini coefficient for points listed in \c vdpidx
/**
Returns a pair holding as first:the Gini Impurity, second: the class votes

\todo handle the case where all the points are classless !
*/
std::pair<double,std::map<ClassVal,uint>>
getGiniImpurity(
	const std::vector<uint>& v_dpidx, ///< datapoint indexes to consider
	const DataSet&           data     ///< dataset
)
{
	START;
	assert( v_dpidx.size()>0 );

	size_t nbClassLess = 0;
	std::map<ClassVal,uint> classVotes; // key: class index, value: votes
	for( auto idx: v_dpidx )
	{
		const auto& dp = data.getDataPoint( idx );
		if( dp.classVal() == ClassVal(-1) )
			nbClassLess++;
		else
			classVotes[ dp.classVal() ]++;
	}
	assert( nbClassLess < v_dpidx.size() );

	double giniCoeff = 1.;
	for( auto elem: classVotes )
	{
		auto v = 1. * elem.second / (v_dpidx.size() - nbClassLess);
		giniCoeff -= v*v;
	}
//	COUT << "global Gini Coeff=" << giniCoeff << '\n';

	return std::make_pair(
		giniCoeff,
		classVotes
	);
}
//---------------------------------------------------------------------
#if 0
// DEPRECATED
/// Returns the class that is in majority in the points defined in \c vIdx.
/// The second value is the percentage of that majority, in the range \f$ [0,1]\f$  (well, \f$ ]0.5,1]\f$  actually !)
//template<typename T>
std::pair<ClassVal,float>
getMajorityClass( const std::vector<uint>& vIdx, const DataSet& data )
{
	START;
	using Pair = std::pair<ClassVal,uint>;

	auto classVotes = computeClassVotes( vIdx, data );

	auto it_max = std::max_element(
		std::begin( classVotes ),
		std::end( classVotes ),
		[]                                      // lambda
		(const Pair& a, const Pair& b)->bool
		{ return a.second < b.second; }
	);

	auto idx_maj = it_max->first;

	return std::make_pair(
		idx_maj,
		1. * classVotes[idx_maj] / vIdx.size()
	);
}
#endif

//---------------------------------------------------------------------
/// Describes how a node holds different classes, see getNodeContent()
struct NodeContent
{
	double   GiniImpurity = 0.;
	ClassVal dominantClass;
	size_t   datasize = 0u;
	size_t   nbPtsOtherClasses = 0u;
	size_t   nbClasses = 0u;

	friend std::ostream& operator << ( std::ostream& f, const NodeContent& nc )
	{
		f << "NodeContent: "
		<< " GiniImpurity="      << nc.GiniImpurity
		<< " dominantClass="     << nc.dominantClass
		<< " datasize="          << nc.datasize
		<< " nbPtsOtherClasses=" << nc.nbPtsOtherClasses
		<< " nbClasses="         << nc.nbClasses
		<< '\n';
		return f;
	}
};

//---------------------------------------------------------------------
/// Returns some info on what a node holding the points defined by \c v_dpidx holds.
/**
\todo Here, we iterate twice on the set, that could probably be done with a single iteration.
*/
NodeContent
getNodeContent(
	const std::vector<uint>& v_dpidx, ///< datapoint indexes to consider
	const DataSet&           data     ///< dataset
)
{
	START;
	auto gImp = getGiniImpurity( v_dpidx, data );

	const auto& classVotes = gImp.second;
//	priv::printMap( std::cout, classVotes, "classvotes" );
//	COUT << "global Gini Impurity=" << gImp.first << '\n';

	using Pair = std::pair<ClassVal,uint>;
	auto it_max = std::max_element(  // search max value based on nb of votes
		std::begin( classVotes ),
		std::end( classVotes ),
		[]                                      // lambda
		(const Pair& a, const Pair& b)->bool
		{ return a.second < b.second; }
	);

	auto idx_maj = it_max->first;
	COUT << "idx_maj=" << idx_maj << "\n";

	return NodeContent{
		gImp.first,
		idx_maj,
		v_dpidx.size(),
		v_dpidx.size() - classVotes.at(idx_maj),
		classVotes.size()
	};
}


//---------------------------------------------------------------------
/// Utility function, sort vector and removes values whose difference is small
/**
This compares all the values and removes those which are "too close".

Say we have the values: <code>4 - 5 - 6 - 6.1 - 7 - 8</code> <br>
We want to remove the value "6.1"

First we compute the range: highest - lowest.
Here, \f$ range = 8 - 4 = 4 \f$ <br>
We check the differences between two consecutive values: <br>
\f$ d = \left| v_i - v_{i+1} \right| \f$ <br>
If \f$ d < coeff * range \f$, then it will be considered as "too close".
*/
size_t
removeDuplicates( std::vector<float>& vec, const Params& params )
{
	auto mm = std::minmax_element( std::begin(vec), std::end(vec) )	;
	auto k = (*mm.second - *mm.first) * params.removalCoeff;

	std::sort( vec.begin(), vec.end() );

// remove all values that are equal
	auto it_end = std::unique(
		std::begin(vec),
		std::end(vec),
		[k]                        // lambda
		( float v1, float v2 )
		{
			if( std::fabs(v1-v2) < k )
				return true;
			return false;
		}
	);
	size_t nb_removal = vec.end() - it_end;
	vec.erase( it_end, vec.end() );
	return nb_removal;
}

//---------------------------------------------------------------------
/// Holds all the data relative to an attribute to be able to select it.
struct AttributeData
{
	uint         _atIndex = 0u;         ///< Absolute attribute index
	float        _gain  = 0.f;          ///< Information gain, will be used to select which attribute we use
	ThresholdVal _threshold;            ///< Threshold value, will be set by training and used to classify
	uint         _nbPtsLessThan = 0u;   ///< Nb of points that are less than the threshold

	AttributeData()
	{}
	AttributeData( uint atIdx, float ig, ThresholdVal tval, uint nbpLT ) :
		_atIndex(atIdx),
		_gain(ig),
		_threshold(tval),
		_nbPtsLessThan(nbpLT)
	{}

	friend std::ostream&operator << ( std::ostream& f, const AttributeData& ad )
	{
		f << "AttributeData: index=" << ad._atIndex
			<< " gain=" << ad._gain
			<< " thres=" << ad._threshold
			<< " nbPointsLessThan=" << ad._nbPtsLessThan
			<< ' ';
		return f;
	}
};

//---------------------------------------------------------------------
/// Compute best threshold for attribute \c atIdx, using the Gini Impurity, for the subset of data given by \c v_dpidx.
/**
\return Returns an object of type AttributeData

Details:
- Uses the Gini impurity coeff: https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
- for details, see
 - https://en.wikipedia.org/wiki/Information_gain_in_decision_trees
 - https://towardsdatascience.com/under-the-hood-decision-tree-454f8581684e
*/
//template<typename T>
AttributeData
computeBestThreshold(
	uint                     atIdx,     ///< attribute index we want to process
	const std::vector<uint>& v_dpidx,   ///< datapoint indexes to consider
	const DataSet&           data,      ///< dataset
	double                   giniCoeff, ///< Global Gini coeff for all the points
	const Params&            params     ///< run-time parameters
)
{
	START;
//	LOG(3, "Searching best threshold for attrib=" << atIdx );

// step 1 - compute all the potential threshold values (mean value between two consecutive attribute values)

	std::vector<float> v_attribVal( v_dpidx.size() ); // pre-allocate vector size (faster than push_back)
	for( size_t i=0; i<v_dpidx.size(); i++ )
		v_attribVal[i] = data.getDataPoint( v_dpidx[i] ).attribVal( atIdx );

	auto nbRemoval = removeDuplicates( v_attribVal, params );
//	std::cout << "Removal of " << nbRemoval << " attribute values\n";

	if( v_attribVal.size() < 2 )         // if only one value, is pointless
	{
		std::cout << "WARNING, unable to compute best threshold value for attribute " << atIdx
			<< ", maybe check value of 'removalCoeff'\n";
		return AttributeData();
	}

	std::vector<float> v_thresVal( v_attribVal.size()-1 ); // if 10 values, then only 9 thresholds
	for( uint i=0; i<v_thresVal.size(); i++ )
		v_thresVal[i] = ( v_attribVal.at(i) + v_attribVal.at(i+1) ) / 2.f; // threshold is mean value between the 2 attribute values

	LOG(3, "Searching best threshold for attrib=" << atIdx << " among " << v_thresVal.size() << " thresholds, based on " << v_dpidx.size() << " pts" );

// step 2: compute IG for each threshold value

	std::vector<float> deltaGini( v_thresVal.size() );   // one value per threshold
	std::vector<uint> nb_LT( v_thresVal.size(), 0u );    // will hold the nb of points lying below the threshold
	for( size_t i=0; i<v_thresVal.size(); i++ )          // for each threshold value
	{
//		COUT << "thres " << i << "=" << v_thresVal[i] << '\n';
		std::map<ClassVal,uint> m_LT, m_HT;

		uint nb_HT = 0;
		for( auto ptIdx: v_dpidx )                         // for each data point
		{
			const auto& point = data.getDataPoint(ptIdx);
			auto attribVal = point.attribVal( atIdx );
			if( attribVal < v_thresVal[i] )
			{
				m_LT[ point.classVal() ]++;
				nb_LT[i]++;
			}
			else
			{
				m_HT[ point.classVal() ]++;
				nb_HT++;
			}
		}

		auto g_LT = 1.;
		for( auto p: m_LT )  // for the values that are Lower Than the threshold
		{
			auto val = 1. * p.second / nb_LT[i];
			g_LT -= val*val;
		}
		auto g_HT = 1.;

		for( auto p: m_HT )  // for the values that are Higher Than the threshold
		{
			auto val = 1. * p.second / nb_HT;
			g_HT -= val*val;
		}
		deltaGini[i] = giniCoeff - (g_LT + g_HT) / 2.;
	}

// step 3 - find max value of the delta Gini
	auto max_pos = std::max_element( std::begin( deltaGini ), std::end( deltaGini ) );

//	COUT << "max gini for thres idx=" << std::distance( std::begin( deltaGini ), max_pos ) << " val=" << *max_pos
//		<< " thresval=" << v_thresVal.at( std::distance( std::begin( deltaGini ), max_pos ) ) << "\n";

	auto best_thres_idx = std::distance( std::begin( deltaGini ), max_pos );

	return AttributeData(
		atIdx,
		*max_pos,
		ThresholdVal(v_thresVal.at( best_thres_idx ) ),
		nb_LT.at( best_thres_idx )
	);
}
//---------------------------------------------------------------------
/// Wrapper around a map holding a bool for each attribute index.
/// Used to check if an attribute has been already used or not.
/**
 * Benefit: has automatic initialization
*/
struct AttribMap
{
	private:
		std::map<uint,bool> _attribMap;
	public:
		AttribMap( uint nbAttribs )
		{
			for( uint i=0; i<nbAttribs; i++ )
				_attribMap[i] = false;
		}
/*		std::vector<uint> getUnusedAttribs() const
		{
			std::vector<uint> vout;
			for( auto elem: _attribMap )
				if( elem.second == false )
					vout.push_back(elem.first);
			return vout;
		}*/
		const std::map<uint,bool>& getMap() const
		{
			return _attribMap;
		}
/// Set attribute \c idx as used, so we will not use it again
/// \todo maybe add some checking here...
		void setAsUsed( uint idx )
		{
			_attribMap[idx] = true;
		}
		const std::map<uint,bool>::const_iterator begin() const
		{
			return std::begin( _attribMap );
		}
		const std::map<uint,bool>::const_iterator end() const
		{
			return std::end( _attribMap );
		}
/// Returns number of unused attributes
		uint nbUnusedAttribs() const
		{
			uint c=0;
			for( const auto& elem: _attribMap )
				if( elem.second == false )
					c++;
			return c;
		}
};
//---------------------------------------------------------------------
/// Finds the best attributes to use, considering the data points of the current node
/// and compute threshold on that attribute so that the two classes are separated at best.
/**
\return A pair holding 1-the index of this attribute and 2-the corresponding threshold value
\return AttributeData
*/
//template<typename T>
AttributeData
findBestAttribute(
	const std::vector<uint>& vIdx,   ///< indexes of data points we need to consider
	const DataSet&           data,   ///< whole dataset
	const Params&            params, ///< parameters
	const AttribMap&         atMap   ///< Search will be limited to the attributes defined here
)
{
	START;
	assert( atMap.nbUnusedAttribs() != 0 );

	LOG( 2, "Searching best attribute among " << atMap.nbUnusedAttribs() );

	auto giniCoeff = getGiniImpurity( vIdx, data );

// step 1 - compute best IG/threshold for each attribute, only for the considered points
	std::vector<AttributeData> v_IG;
	for( const auto atIdx: atMap.getMap() )
		if( atIdx.second == false )
			v_IG.push_back( computeBestThreshold( atIdx.first, vIdx, data, giniCoeff.first, params ) );

// step 3 - get the one with max gain value
	auto it_mval = std::max_element(
		std::begin(v_IG),
		std::end(v_IG),
		[]                         // lambda
		( const AttributeData& p1, const AttributeData& p2 )
		{
			return p1._gain < p2._gain;
		}
	);

	LOG( 2, "highest GI with attribute " << it_mval->_atIndex << ", GI=" << it_mval->_gain );

	return *it_mval;
}

//---------------------------------------------------------------------
/// Recursive helper function, used by TrainingTree::train()
/**
Computes the threshold, splits the dataset and assigns the split to 2 sub nodes (that get created)
*/
////template<typename T>
void
splitNode(
	vertexT_t         v,         ///< current node id
	GraphT&           graph,     ///< graph
	const DataSet&    data,      ///< dataset
	const Params&     params     ///< parameters
)
{
	START;
	static uint s_recDepth;
	s_recDepth++;

//	static uint s_nodeId = params.initialVertexId;   // starts at one because the initial node (id=0) is created elsewhere

	const auto& vIdx = graph[v].v_Idx; // vector holding the indexes of the datapoints for this node
	LOG( 1, "splitting node " << graph[v]._nodeId << " depth=" << s_recDepth << ", holding " << vIdx.size() << " points" );

// step 1.1 - check if there are different output classes in the given data points
// if not, then we are done

	auto nodeContent = getNodeContent( vIdx, data );
//	COUT << nodeContent;

	auto giniImpurity = nodeContent.GiniImpurity;

	graph[v]._class = nodeContent.dominantClass;
	graph[v].giniImpurity = giniImpurity;

	if( s_recDepth>params.maxTreeDepth )
	{
		LOG( 1, "tree reached max depth (=" << params.maxTreeDepth << "), STOP" );
		s_recDepth--;
		graph[v]._type = NT_Final_MD;
		return;
	}

	if( giniImpurity < params.minGiniCoeffForSplitting )
	{
		LOG( 1, "dataset is (almost or completely) pure, gini coeff=" << giniImpurity << ", STOP" );
		s_recDepth--;
		graph[v]._type = NT_Final_GI_Small;
		return;
	}

	AttribMap aMap( data.nbAttribs() );
	AttributeData bestAttrib;
	bool done = false;
	do
	{

	// step 2 - find the best attribute to use to split the data, considering the data points of the current node
		bestAttrib = findBestAttribute( vIdx, data, params, aMap );
//		COUT << "best attrib:" << bestAttrib << "\n";

		aMap.setAsUsed( bestAttrib._atIndex );
	// before splitting, make sure that one of the childs will not have an insufficient number of points
		auto n1= bestAttrib._nbPtsLessThan;
		auto n2 = vIdx.size() - n1;
		if( n1 < params.minNbPoints || n2 < params.minNbPoints )
		{
			LOG( 1, "not enough points if splitting on attribute " << bestAttrib._atIndex << ": n1=" << n1 << " n2=" << n2 << ", trying next attribute" );

			if( aMap.nbUnusedAttribs() == 0 )
			{
				LOG( 1, "no more attributes to try, STOP" );
				s_recDepth--;
				graph[v]._type = NT_Final_SplitTooSmall;
				return;
			}
		}
		else
			done = true;
	}
	while( !done );
//
// !!! from here, a split will occur !!!
//
	graph[v]._attrIndex = bestAttrib._atIndex;
	graph[v]._threshold = bestAttrib._threshold.get();
	graph[v].giniImpurity = -1.f;
	if( graph[v]._type != NT_Root )   // so the root... stays the root !
		graph[v]._type = NT_Decision;

// step 3 - different classes here: we create two child nodes and split the dataset
	auto v1 = boost::add_vertex(graph);
	auto v2 = boost::add_vertex(graph);

	graph[v1].depth = graph[v].depth+1;
	graph[v2].depth = graph[v].depth+1;

	auto et = boost::add_edge( v, v1, graph );
	auto ef = boost::add_edge( v, v2, graph );
	graph[et.first].edgeSide = true;
	graph[ef.first].edgeSide = false;

	COUT << "two nodes added, total nb=" << boost::num_vertices(graph) << "\n";

	graph[v1].v_Idx.reserve( vIdx.size() );
	graph[v2].v_Idx.reserve( vIdx.size() );
	for( auto idx: vIdx )           // separate the data points into two sets
	{
		auto attrVal = data.getDataPoint( idx ).attribVal( bestAttrib._atIndex );
		if( attrVal < bestAttrib._threshold.get() )
			graph[v1].v_Idx.push_back( idx );
		else
			graph[v2].v_Idx.push_back( idx );
	}
	COUT << "after split: v1:"<< graph[v1].v_Idx.size() << " points, v2:"<< graph[v2].v_Idx.size() << " points\n";

	if( graph[v1].v_Idx.size() )
		splitNode( v1, graph, data, params );

	if( graph[v2].v_Idx.size() )
		splitNode( v2, graph, data, params );

	s_recDepth--;
}
//---------------------------------------------------------------------
/// Pruning of the graph: removal of child leaves pair that hold the same class
/**

Algorithm:
\verbatim
DO
	iterate through the vertices, until we have a removal:
		IF vertex it is a leave:
		THEN
			go up one step and check other child
			IF other child is a leave AND has same class
			THEN
				Merge the two vertices:
					tag parent as Leave
					remove the two childs
				removal = true
WHILE( no more removals )
\endverbatim

\note Could have tried something else:
check each node one by one and find if there is another node of same depth AND same class
that has the same parent.
*/

size_t
TrainingTree::pruning()
{
	START;

	std::set<uint> nodeSet;
	LOG( 1, "start pruning, nb nodes=" + std::to_string( boost::num_vertices( _graph ) ) );

	size_t iter = 0;
	size_t nbRemoval = 0;
	bool removalHappened = false;
	do{
//		std::cerr << "iter=" << iter++ << " nb vertices=" << boost::num_vertices( _graph ) << std::endl;
		removalHappened = false;
	for(
		auto pit = boost::vertices( _graph );     // iterate on
		pit.first != pit.second;                  // all the vertices
		pit.first++
	)
	{
		auto v1    = *pit.first;
		auto node1 = _graph[v1];
//		std::cerr << "node1=" << node1._nodeId << " class=" << (int)node1._type << std::endl;
		nodeSet.insert( node1._nodeId );
		if( node1.isLeave() ) // but only care about the leaves
		{
//			std::cerr << " -is leave" << std::endl;
			assert( boost::in_degree( v1, _graph )  == 1 );
			auto pe_in = boost::in_edges( v1, _graph );       // get the ingoing edges (only 1 actually)
			auto v0 = boost::source( *pe_in.first, _graph );  // get source vertex
			assert( boost::out_degree( v0, _graph ) == 2 );

			auto pedges = boost::out_edges( v0, _graph );
			auto eit1 = pedges.first++;
			auto eit2 = pedges.first;

			edge_t other = *eit1;
			if( other == *pe_in.first )
				other = *eit2;

			auto v2 = boost::target( other, _graph );
			auto node2 = _graph[v2];
//			std::cerr << " - node2=" << node2._nodeId << std::endl;
			if( nodeSet.find( node2._nodeId ) == nodeSet.end() )  // if not already parsed
			{
				nodeSet.insert( node2._nodeId );
				if( node2.isLeave() )                             // and is a leave of the tree
					if( node1._class == node2._class )             // if same class !
					{
						LOG( 1, "removing nodes " + std::to_string(node1._nodeId) + " and " + std::to_string(node2._nodeId) );
						auto& node0 = _graph[v0];
						node0._class = node1._class;  // change status of source node
						node0._type = NT_Merged;
						boost::clear_vertex( v1, _graph );
						boost::clear_vertex( v2, _graph );
						boost::remove_vertex( v1, _graph );
						boost::remove_vertex( v2, _graph );
						nbRemoval++;
//						std::cerr << " - nbRemoval=" << nbRemoval << std::endl;
						removalHappened = true;
						break;
					}
			}
		}
	}
//	std::cerr << " - end of loop" << std::endl;
	}
	while( removalHappened );
/*
	LOG( 1, "AFTER PRUNING, nb nodes=" + std::to_string( boost::num_vertices(_graph)) );
	for(
		auto pit = boost::vertices( _graph );     // iterate on
		pit.first != pit.second;                  // all the vertices
		pit.first++
	)
	{
		auto v = *pit.first;
		auto node= _graph[v];
		COUT << "Id=" << node._nodeId << " type=" << (int)node._type << "\n";
	}
	COUT << "nbRemoval=" << nbRemoval << '\n';
*/
	return nbRemoval;
}
//---------------------------------------------------------------------
/// Train tree using data.
/**
\return false if failure
*/
//template<typename T>
void
TrainingTree::train( const DataSet& data, const Params params )
{
	START;
	TIMER_START;
	LOG( 0, "Start training" );

	NodeT::resetNodeId();
	auto nbAttribs = data.nbAttribs();
	if( !nbAttribs )
		throw std::runtime_error( "no attributes!" );
	if( data.size()<2 )
		throw std::runtime_error( "no enough data points!" );

	_initialVertex = boost::add_vertex(_graph);  // create initial vertex

	std::vector<uint> v_idx;
	v_idx.reserve( data.size() );
	if( data.nbOutliers() )
	{
		for( size_t i=0; i<data.size(); i++ )
			if( !data.pointIsOutlier(i) )
				v_idx.push_back( i );
	}
	else
	{
		v_idx.resize( data.size() );  // create vector holding indexes of all the data points
		std::iota( v_idx.begin(), v_idx.end(), 0 );
	}

	_graph[_initialVertex].v_Idx = v_idx;
	_graph[_initialVertex]._type = NT_Root;

	splitNode( _initialVertex, _graph, data, params ); // Call the "split" function (recursive)
	LOG( 0, "Training done" );
}

//---------------------------------------------------------------------
/// Returns class of data point as classified by tree
//template<typename T>
ClassVal
TrainingTree::classify( const DataPoint& point ) const
{
	ClassVal retval{-1};
	vertexT_t v = _initialVertex;   // initialize to first node
	bool done = false;
//	uint iter = 0;
	do
	{
		assert( _graph[v]._type != NT_undef );
//		COUT << "\n*iter=" << iter++ << " NODE " << _graph[v]._nodeId << std::endl;
		if( _graph[v]._type != NT_Root && _graph[v]._type != NT_Decision ) // then, we are done !
		{
			done = true;
			retval = _graph[v]._class;
//			COUT << "Final node: class = " << retval << "\n";
		}
		else
		{
			auto attrIndex = _graph[v]._attrIndex;  // get attrib index that this node handles
			auto atValue   = point.attribVal( attrIndex );  // get data point value for this attribute
//			COUT << "Considering attribute " << attrIndex << " with value " << atValue << std::endl;
			assert( boost::out_degree( v, _graph ) == 2 );

			auto edges = boost::out_edges( v, _graph );    // get the two output edges of the node
			auto et = edges.first++;
			auto ef = edges.first;
			if( _graph[*ef].edgeSide )
				std::swap( et, ef );

//			COUT << "node thres=" << _graph[v]._threshold << std::endl;
			if( atValue < _graph[v]._threshold )  // depending on threshold, define the next node
				v = boost::target( *et, _graph );
			else
				v = boost::target( *ef, _graph );
		}
	}
	while( !done );
	return retval;
}

//---------------------------------------------------------------------
/// Classify \c dataset and returns performance score
ConfusionMatrix
TrainingTree::classify( const DataSet& dataset ) const
{
	if( _nbClasses < 2 )  // if 0 or 1 class, then nothing to classify
		throw std::runtime_error( "nothing to classify, dataset holds " + std::to_string(_nbClasses) + " classes" );

	ConfusionMatrix confmat( _classIndexMap );
	for( const auto& datapoint: dataset )
	{
		auto cla1 = datapoint.classVal();
		auto cla2 = classify( datapoint );
		if( cla1 != ClassVal(-1) )
			confmat.add( cla1, cla2 );
	}
	return confmat;
}

} // namespace dtcpp
//---------------------------------------------------------------------
#endif // DTCPP_HG
