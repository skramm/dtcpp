@article{Chawla_2002
,title={SMOTE: Synthetic Minority Over-sampling Technique}
,volume={16}
,ISSN={1076-9757}
,url={http://dx.doi.org/10.1613/jair.953}
,DOI={10.1613/jair.953}
,journal={Journal of Artificial Intelligence Research}
,publisher={AI Access Foundation}
,author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.}
,year={2002}
,month={Jun}
,pages={321–357}
}


@article{Wong_2015
,title = {Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation}
,journal = {Pattern Recognition}
,volume = {48}
,number = {9}
,pages = {2839-2846}
,year = {2015}
,issn = {0031-3203}
,doi = {https://doi.org/10.1016/j.patcog.2015.03.009}
,url = {https://www.sciencedirect.com/science/article/pii/S0031320315000989}
,author = {Tzu-Tsung Wong}
,keywords = {Classification, Independence, -Fold cross validation, Leave-one-out cross validation, Sampling distribution},
,abstract = {Classification is an essential task for predicting the class values of new instances. Both k-fold and leave-one-out cross validation are very popular for evaluating the performance of classification algorithms. Many data mining literatures introduce the operations for these two kinds of cross validation and the statistical methods that can be used to analyze the resulting accuracies of algorithms, while those contents are generally not all consistent. Analysts can therefore be confused in performing a cross validation procedure. In this paper, the independence assumptions in cross validation are introduced, and the circumstances that satisfy the assumptions are also addressed. The independence assumptions are then used to derive the sampling distributions of the point estimators for k-fold and leave-one-out cross validation. The cross validation procedure to have such sampling distributions is discussed to provide new insights in evaluating the performance of classification algorithms.}
}

@article{Kautz_2017
,author = {Kautz, Thomas and Eskofier, Bjoern M. and Pasluosta, Cristian F.}
,title = {Generic Performance Measure for Multiclass-Classifiers},
,year = {2017}
,publisher = {Elsevier Science Inc.}
,volume = {68}
,number = {C}
,issn = {0031-3203}
,abstract = {Generic, compact and meaningful performance measure for arbitrary classifiers.Includes confidence indicators in interval [0; 1].Robust towards class imbalance, sensitive towards class separation.Demo implementation available. The evaluation of classification performance is crucial for algorithm and model selection. However, a performance measure for multiclass classification problems (i.e., more than two classes) has not yet been fully adopted in the pattern recognition and machine learning community. In this work, we introduce the multiclass performance score (MPS), a generic performance measure for multiclass problems. The MPS was designed to evaluate any multiclass classification algorithm for any arbitrary testing condition. This measure handles the case of unknown misclassification costs and imbalanced data, and provides confidence indicators of the performance estimation. We evaluated the MPS using real and synthetic data, and compared it against other frequently used performance measures. The results suggest that the proposed MPS allows capturing the performance of a classification with minimum influence from the training and testing conditions. This is demonstrated by its robustness towards imbalanced data and its sensitivity towards class separation in feature space.},
,journal = {Pattern Recognition.}
,month = aug
,pages = {111–125}
,numpages = {15}
,keywords = {Classification, Evaluation, Model selection, Performance, Multiclass, Accuracy}
,url={https://dl.acm.org/doi/abs/10.5555/3085601.3085714}
}

@article{Kotsiantis_2013
,author={Kotsiantis, S. B.}
,year={2013}
,url={https://doi.org/10.1007/s10462-011-9272-4}
,title={Decision trees: a recent overview}
,journal={JO  - Artificial Intelligence Review}
,volume={39}
,pages={261–283}
,abstract = {Decision tree techniques have been widely used to build classification models as such models closely resemble human reasoning and are easy to understand. This paper describes basic decision tree issues and current research points. Of course, a single article cannot be a complete review of all algorithms (also known induction classification trees), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored}
}

@article{Sokolova_2009
,author={Marina Sokolova and Guy Lapalme}
,year={2009}
,title={"A systematic analysis of performance measures for classification tasks"}
,journal={Information Processing and Management}
,volume={45}
,pages={427-437}
,url={https://doi.org/10.1016/j.ipm.2009.03.002}
,abstract={This paper presents a systematic analysis of twenty four performance measures used in the
complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class,
multi-labelled, and hierarchical. For each classification task, the study relates a set of
changes in a confusion matrix to specific characteristics of data. Then the analysis concen-
trates on the type of changes to a confusion matrix that do not change a measure, therefore,
preserve a classifier’s evaluation (measure invariance). The result is the measure invariance
taxonomy with respect to all relevant label distribution changes in a classification problem.
This formal analysis is supported by examples of applications where invariance properties
of measures lead to a more reliable evaluation of classifiers. Text classification supplements
the discussion with several case studies.}
}

@article{Wu_2008
,year={2008}
,author={Wu, X. and Kumar, V. and Ross Quinlan J. et al.}
,title={Top 10 algorithms in data mining}
,journal={Knowl Inf Syst}
number={14}
,pages={1-37}
,url={https://doi.org/10.1007/s10115-007-0114-2}
}

@article{Ben-Gal_2014
,author = {Irad Ben-Gal and Alexandra Dana and Niv Shkolnik and Gonen Singer}
,title = {Efficient Construction of Decision Trees by the Dual Information Distance Method}
,journal = {Quality Technology \& Quantitative Management}
,volume = {11}
,number = {1}
,pages = {133-147}
,year  = {2014}
,publisher = {Taylor & Francis}
,doi = {10.1080/16843703.2014.11673330}
,URL = {https://doi.org/10.1080/16843703.2014.11673330}
,abstract={The construction of efficient decision and classification trees is a fundamental task in Big Data
analytics which is known to be NP-hard. Accordingly, many greedy heuristics were suggested for the construction
of decision-trees, but were found to result in local-optimum solutions. In this work we present the dual
information distance (DID) method for efficient construction of decision trees that is computationally attractive,
yet relatively robust to noise. The DID heuristic selects features by considering both their immediate contribution
to the classification, as well as their future potential effects. It represents the construction of classification
trees by finding the shortest paths over a graph of partitions that are defined by the selected features. The DID
method takes into account both the orthogonality between the selected partitions, as well as the reduction of
uncertainty on the class partition given the selected attributes. We show that the DID method often outperforms
popular classifiers, in terms of average depth and classification accuracy.}
}


@book{B_Kuhn_2018
,title={Applied Predictive Modeling}
,author={Max Kuhn and Kjell Johnson}
,year={2018}
,isbn={978-1461468486}
,publisher={Springer}
}

@book{B_James_2021
,title={An Introduction to Statistical Learning: with Applications in R}
,author={Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani}
,year={2021}
,isbn={978-1071614174}
,publisher={Springer}
}

@book{B_Russell_2020
,title={Artificial Intelligence: A Modern Approach}
,author={Stuart Russell and Peter Norvig}
,year={2020}
,isbn={978-0134610993}
,publisher={Pearson}
}

@book{B_Imbalanced Learning_2013
,title={Imbalanced Learning: Foundations, Algorithms, and Applications
,editor={Haibo He and Yunqian Ma}
,publisher={Wiley-IEEE Press}
,isbn={978-1118074626}
}

@book{B_Shalev_2014
,title={Understanding Machine Learning,From Theory to Algorithms}
,author={Shai Shalev-Shwartzand Shai Ben-David}
,year={2014}
,isbn={9781107057135}
,publisher={Cambridge University Press}
}
